---
title: "Sleepers"
author: "Danijel Kopčinović"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sleepers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

In this vignette we will present the analysis of the survey data provided by Gradient Metrics. The data was gathered to find out the best marketing message for the mobile application that helps people with sleeping problems.

The parts of the message were organized into groups ("attributes") as follows:


```{r, echo = FALSE, error = FALSE, warning = FALSE}

#library(sleepers)
library(haven)
library(dplyr)

experiment_data = read_sav('../inst/extdata/experiment_data.sav')

# perform data adjustment, factorization, normalization...

# factorize all columns except task, duration and price
fact_columns = setdiff(colnames(experiment_data), c("task", "duration", "price"))
experiment_data[, fact_columns] = lapply(experiment_data[, fact_columns], factor)

# now factorize duration and price, but create new columns, this is just for pretty printing
experiment_data$duration_f = factor(experiment_data$duration)
experiment_data$price_f = factor(experiment_data$price)

# save factorized attributes columns names for later usage
att_columns = setdiff(colnames(experiment_data), c("response_id", "task", "duration", "price", "answer"))

# cast duration to numeric and normalize it to 0-1 range
# duration has 3 values c("3 months", "6 months", "12 months") which are not equidistant
# we want to capture this difference between the values and allow for other values too
experiment_data$duration = as.numeric(unlist(lapply(str_split(experiment_data$duration, "[[:space:]]+month(s)*"), "[[", 1)))
min_experiment_data_duration = min(experiment_data$duration)
max_experiment_data_duration = max(experiment_data$duration)
experiment_data$duration = (experiment_data$duration - min_experiment_data_duration)/(max_experiment_data_duration - min_experiment_data_duration)

# cast price to numeric and normalize it to 0-1 range
# price has 3 values c("$20/month", "$30/month", "$40/month") and these are equidistant
# we want to capture the ordering of these values and allow for other values too
experiment_data$price = as.numeric(gsub("\\$", "", unlist(lapply(str_split(experiment_data$price, "/month(s)*"), "[[", 1))))
min_experiment_data_price = min(experiment_data$price)
max_experiment_data_price = max(experiment_data$price)
experiment_data$price = (experiment_data$price - min_experiment_data_price)/(max_experiment_data_price - min_experiment_data_price)

# answer has only numbers and we want to have text descriptions
levels(experiment_data$answer) = c("Very unlikely", "Somewhat unlikely", "Somewhat likely", "Very likely")
experiment_data$answer = factor(experiment_data$answer, levels = c("Very unlikely", "Somewhat unlikely", "Somewhat likely", "Very likely"), ordered = TRUE)

# IMPORTANT: create a predict_transform function that will take new data given in the same format as 
# experiment_data and perform adjustments as described above to be able to make predictions.
predict_transform = function(new_data) {
  for (cn in setdiff(fact_columns, "answer")) {
    new_data[[cn]] = factor(new_data[[cn]], levels = levels(experiment_data[[cn]]))
  }
  new_data$duration = as.numeric(unlist(lapply(str_split(new_data$duration, "[[:space:]]+month(s)*"), "[[", 1)))
  new_data$duration = (new_data$duration - min_experiment_data_duration)/(max_experiment_data_duration - min_experiment_data_duration)
  new_data$price = as.numeric(gsub("\\$", "", unlist(lapply(str_split(new_data$price, "/month(s)*"), "[[", 1))))
  new_data$price = (new_data$price - min_experiment_data_price)/(max_experiment_data_price - min_experiment_data_price)
  new_data
}

# printout attributes and values
r = lapply(att_columns, function(cn) {
  cn_levels = levels(experiment_data[[cn]])
  print(paste0("Attribute ", cn, " (", length(cn_levels), " values): ", paste(cn_levels, collapse = " | ")))
})

# just checking if every respondent answered the same number of questions > YES
# respondent_tasks = experiment_data %>% group_by(response_id) %>% summarise(max_task = max(task))
# max(respondent_tasks$max_task)-min(respondent_tasks$max_task)

```

There were `r length(levels(experiment_data$response_id))` respondents and each respondent answered `r max(experiment_data$task)` questions.

Each question had `r length(levels(experiment_data$answer))` answer options: `r paste0(levels(experiment_data$answer), collapse = " | ")`. Respondent chose one answer option on each question.

Let's have a look at how many times each attribute was shown and the frequency distribution of the answers:

```{r, echo = FALSE, error = FALSE, warning = FALSE}

summary(experiment_data[, c(att_columns, "answer")])
# here we also see that we don't have any NAs, missing data

```


Besides the survey about the best marketing message for the mobile application, respondents also filled a personal survey, giving information about them, their sleeping problems, ways of coping with them etc.


```{r, echo = FALSE, error = FALSE, warning = FALSE}

survey_data = read_sav('../inst/extdata/survey_data.sav')
# factorize all columns
survey_data_1 = data.frame(lapply(survey_data, factor))
# levels are now numeric, we want to have text descriptions on what each level means
fact_columns_names = colnames(survey_data)[colnames(survey_data) != "response_id"]
for (cn in fact_columns_names) {
  # have to make this check because some labels were NULL, unclear why
  if (!is.null(attributes(survey_data[[cn]])$labels)) {
    levels(survey_data_1[[cn]]) = names(attributes(survey_data[[cn]])$labels)
  }
}
survey_data = survey_data_1


ftable(xtabs(~ social_proof + price_f + answer, data = experiment_data))


```


There were `r ncol(survey_data)` additional questions in this personal survey. With this additional data, we tried to segment the respondents (with respect to the answers they gave) and identify a segment/group that would be more willing to buy the mobile application.


# Experiment Data Analysis and Modelling


As a first step in our analysis of the (experimental) survey data, we will make a quick "counting" analysis: let's see what is the average "buying intent" (from 1 = `r levels(experiment_data$answer)[1]` to `r length(levels(experiment_data$answer))` = `r levels(experiment_data$answer)[length(levels(experiment_data$answer))]`) for different attributes and their values.


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

library(ggplot2)
library(gridExtra)

cutlabels10 = function(x) {
  is_long = nchar(x) > 10
  x[is_long] = paste0(substr(x[is_long], 1, 7), "...")
  x
}

plot_rvar = function(rvar, df) {
  rank_1 = df[!is.na(df[[rvar]]), ] %>% group_by(get(rvar)) %>% summarise(avg_buying_intent = round(mean(as.numeric(answer)), 2))
  colnames(rank_1)[1] = rvar
  label_fill = rep("gray", length(rank_1$avg_buying_intent))
  label_fill[rank_1$avg_buying_intent == max(rank_1$avg_buying_intent)] = "red"
  label_fill[rank_1$avg_buying_intent == min(rank_1$avg_buying_intent)] = "pink"
  ggplot(data = rank_1, aes(x = get(rvar), y = avg_buying_intent)) + scale_x_discrete(labels = cutlabels10) + geom_col(fill = label_fill) + geom_text(aes(x = get(rvar), y = avg_buying_intent/2, label = rank_1$avg_buying_intent), data = rank_1, size = 3, colour = "white") + xlab(rvar) + theme(axis.text.x = element_text(size = 8), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
}
ggi = lapply(att_columns, plot_rvar, df = experiment_data)
marrangeGrob(ggi, nrow = 3, ncol = 2)

```


We can see that the *price* attribute levels show relatively clear trend (higher buying intent for lower price, lower buying intent for higher price). This is expected and shows that we don't have strange behavior. 

With other attributes, we can't see such a clear trend.

Let's also have a look at the average buying intents for different 2-combinations of attribute levels:


```{r echo= FALSE, warning = FALSE, message = FALSE}

two_atts_combs = combn(x = att_columns, m = 2)
ggi2 = lapply(1:ncol(two_atts_combs), function(two_atts_i) {
  two_atts = two_atts_combs[, two_atts_i]
  fst_att = two_atts[1]
  snd_att = two_atts[2]
  #print(paste0(fst_att, ", ", snd_att))
  fst_levs = levels(experiment_data[[fst_att]])
  snd_levs = levels(experiment_data[[snd_att]])
  rank_1 = experiment_data[!is.na(experiment_data[[fst_att]]) & !is.na(experiment_data[[snd_att]]), ] %>% group_by(get(fst_att), get(snd_att)) %>% summarise(avg_buying_intent = round(mean(as.numeric(answer)), 2))
  if (nrow(rank_1) == 0) return(NULL)
  colnames(rank_1)[1:2] = c(fst_att, snd_att)
  label_fill = rep("gray", length(rank_1$avg_buying_intent))
  label_fill[rank_1$avg_buying_intent == max(rank_1$avg_buying_intent)] = "red"
  label_fill[rank_1$avg_buying_intent == min(rank_1$avg_buying_intent)] = "pink"
  ggplot(data = rank_1, aes(x = get(fst_att), y = get(snd_att))) + scale_x_discrete(labels = cutlabels10) + scale_y_discrete(labels = cutlabels10) + geom_label(aes(x = get(fst_att), y = get(snd_att), label = avg_buying_intent), data = rank_1, colour = "white", fill = label_fill) + xlab(fst_att)+ ylab(snd_att) + theme(axis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8))
})
ggi2 = ggi2[!sapply(ggi2, is.null)]
marrangeGrob(ggi2, nrow = 2, ncol = 2)

#grid.arrange(ggi[[1]], ggi[[2]], ggi[[3]], ggi[[4]], ggi[[5]], ggi[[6]], ggi[[7]], ggi[[8]], ggi[[9]], ncol = 2)

```


We can see that the following combinations of attributes have somewhat bigger difference between the smallest (pink) and largest (red) average buying intent: *duration + price, offer + price, offer + rtb, offer + social_proof and outcome + price*. This indicates that some of these value combinations could have an interaction effect, where one value combined with another value gives a cumulative (positive or negative) effect on the average buying intent. Thus maybe we should include these combinations in the modeling.

It is now time to build a model. Since our output variable *answer* is an ordinal variable (has comparable values), we will use an ordinal regression model with *answer* as the output variable and all other variables as predictors. 

We get the following summary:


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

library(MASS)

options(contrasts = c("contr.treatment", "contr.poly"))
sleepers.plr <- polr(answer ~ duration + offer + outcome + price + rtb + social_proof, data = experiment_data, Hess = TRUE)

# save the model 
saveRDS(sleepers.plr, "model_sleepers.plr.RDS")

#logLik(sleepers.plr)

# all coefficients together, intercepts too, with estimate, std. error and t value
sumtable = data.frame((summary(sleepers.plr))$coefficients)
# last few coefficients are intercepts, let's emphasize that
intercepts_rows = nrow(sumtable) - ((length(sleepers.plr$zeta) - 1):0)
rownames(sumtable)[intercepts_rows] = paste0("Interc_", rownames(sumtable)[intercepts_rows])
# now adding p-values and significance, it is a usual way to display the results, don't know why polr doesn't have it
sumtable$p.value = pt(abs(sumtable$t.value), sleepers.plr$nobs - sleepers.plr$edf, lower.tail = FALSE) * 2
sumtable$signif = ifelse(sumtable$p.value < .1, ".", "")
sumtable[sumtable$p.value < .05, ]$signif = "*"
sumtable[sumtable$p.value < .01, ]$signif = "**"
sumtable[sumtable$p.value < .001, ]$signif = "***"
sumtable$odds_effect = exp(sumtable$Value)
sumtable$explanation = "" # create a column
odds_effect_minus_1 = sumtable$odds_effect - 1
sumtable[odds_effect_minus_1 < 0, ]$explanation = paste0("decrease buying odds by ", round(abs(odds_effect_minus_1[odds_effect_minus_1 < 0]), 2))
sumtable[odds_effect_minus_1 >= 0, ]$explanation = paste0("increase buying odds by ", round(odds_effect_minus_1[odds_effect_minus_1 >= 0], 2))

sumtable_cis = confint(sleepers.plr)

sumtable[, c("Value", "signif", "odds_effect")]

```


Each coefficient represents one attribute level or attribute transformation (".L" for linear, ".Q" for quadratic). Without getting into too many details about this output format, we can see (from *signif* column) that only `r paste0(rownames(sumtable[sumtable$signif != "", ]), collapse = ", ")` coefficients are significant (not 0 with high probability).

Negative coefficients in the table above mean that increasing the corresponding attribute value reduces the odds of the buying intent and vice versa - positive coefficients mean that increasing the corresponding attribute value increases the odds of the buying intent. So e.g. increasing *price* (with its negative coefficient `r sumtable["price", "Value"]`) reduces the buying intent while giving *scientific evidence* in *social_proof* (with its positive coefficient `r sumtable["social_proofscientific evidence", "Value"]`) increases the buying intent compared with the *`r levels(experiment_data$social_proof)[1]`*.

The coefficients ("Value") in the table are given on a logarithmic scale, so to find how they affect the odds of buying intent, we have to exponentiate them. This is the "odds_effect" column. If this number is < 1, the attribute value decreases the odds of buying. If this number is > 1, the attribute value increases the odds of buying.

Taking everything into account, we have the following attribute values with their effect on buying odds as just described:

```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

sumtable[(grepl("Interc", rownames(sumtable)) == FALSE), c("odds_effect", "explanation")]

```


Since we built an ordinal regression model, we have to check its main assumption (parallel regression assumption, a.k.a. the proportional odds assumption).


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

library(brant)
brant(sleepers.plr)

```


Looking at the probability column (it's the 3rd number in each row, some are moved to left because of an error in tabulating the output), we can see that all coefficients except price have probability of the assumption H0 above 0.05 (5%). This means that we can trust the H0 and the modeling assumption holds for these coefficients.

We ignore the Omnibus 0 probability because it's essentially a product of all the other probabilities and thus very small by default.

Price doesn't follow the proportional odds assumption, but this is expected, because influence of the price on the buying intent is not proportional, e.g. a higher price will affect the difference between *Somewhat unlikely* and *Very unlikely* less than the difference between *Very likely* and *Somewhat likely*.

Thus altogether we can proceed with the analysis.

Since a lot of coefficients are non significant, we tested if we can remove some variables from the model or improve the model by adding interactions (as explained before with the graphics).


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

#addterm(sleepers.plr, ~.^2, test = "Chisq")
#sleepers.plr2 <- stepAIC(sleepers.plr, ~ . ^2)
#sleepers.plr2$anova
#anova(sleepers.plr, sleepers.plr2)

```


We found that we could remove *offer* and *duration* but we don't get much improvement (AIC has decreased only around 7). Therefore we rather keep all the variables.


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

#pr <- profile(sleepers.plr)
#confint(pr)
#plot(pr)
#pairs(pr)

```


To check how our model works on the existing data, we ran a check with the following result:


```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

new_data = read_sav('../inst/extdata/experiment_data.sav')
new_data = predict_transform(new_data)
new_data_predicted_probs = predict(sleepers.plr, new_data, type = "p")
new_data_predicted = unlist(lapply(1:nrow(new_data_predicted_probs),function(i) sample(colnames(new_data_predicted_probs), 1, replace = TRUE, prob = new_data_predicted_probs[i, ])))
new_data_predicted = as.numeric(factor(new_data_predicted, levels = levels(experiment_data$answer)))

new_vs_predicted_table = table(new_data_predicted, new_data$answer)

# overall hits
paste0("Overall correct predictions: ", round(100 * psych::tr(new_vs_predicted_table)/sum(new_vs_predicted_table), 2), "%")

```

Summing everything up, the best marketing message would consist of:
* Duration: 3 months
* Offer: improve your sleep sustainably
* Outcome: breaking bad habits and creating new routines
* Price: $20/month
* Rtb: a program created just for you
* Social proof: scientific evidence



# segmentation



```{r, echo = FALSE, error = FALSE, warning = FALSE, message = FALSE}

df = experiment_data
rvar = "response_id"
rank_1 = df[!is.na(df[[rvar]]), ] %>% 
  group_by(get(rvar)) %>% 
  summarise(min_buying_intent = round(min(as.numeric(answer)), 2), 
            avg_buying_intent = round(mean(as.numeric(answer)), 2),
            max_buying_intent = round(max(as.numeric(answer)), 2),)
colnames(rank_1)[1] = rvar
rank_1

sum(rank_1$max_buying_intent == 1) # 239, not buying anything > remove from the dataset, wrong target group
sum(rank_1$min_buying_intent == 4) # 23, buying everything > keep, not so many, perhaps ok target group

sum(rank_1$min_buying_intent == 3) # 112, buyers, these are the most important
sum(rank_1$max_buying_intent == 2) # 151, not buyers, not so important

```


Calculate mean buying intent for each respondent.
Try some clustering algorithms on survey_data. What to do with NAs? If there are too many NAs, remove the column. If NAs can have a meaning, impute data.
Try to find a cluster for which the average buying intent is the highest. Hopefully this will be significantly the highest. Is there some test to see this significance?



# Heading 1
## Heading 2
### Heading 3

--------
********

* Bulleted list
* Item 2
    * Nested bullets need a 4-space indent.
    * Item 2b
* It's possible to put multiple paragraphs of text in a list item.
    
    But to do that, the second and subsequent paragraphs must be
    indented by four or more spaces. It looks better if the first
    bullet is also indented.

1. Item 1.
    * Item a
    * Item b
1. Item 2.


Definition
  : a statement of the exact meaning of a word, especially in a dictionary.
List
  : a number of connected items or names written or printed consecutively,
typically one below the other.
  : barriers enclosing an area for a jousting tournament.

_italic_ or *italic*
__bold__ or **bold**
[link text](destination)<http://this-is-a-raw-url.com>


| Right | Left | Default | Center |
|------:|:-----|---------|:------:|
| 12    | 12   | 12      | 12     |
| 123   | 123  | 123     | 123    |
| 1     | 1    | 1       | 1      |

Notice the use of the : in the spacer under the heading. This determines the alignment of the column.


If the data underlying your table exists in R, don’t lay it out by hand. Instead, use knitr::kable(), or look at printr or pander.

To affect all blocks, call knitr::opts_chunk$set() in a knitr block:
```{r, echo = FALSE}
knitr::opts_chunk$set(
 echo = FALSE,
 warning = FALSE,
 message = FALSE,
 collapse = TRUE, 
 comment = "#>"
)
```


read_sav(system.file("extdata", "experiment_data.sav", package = "sleepers"))
read_sav(system.file("extdata", "survey_data.sav", package = "sleepers"))





